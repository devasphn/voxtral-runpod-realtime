# COMPLETELY FIXED - main.py - CONTINUOUS STREAMING WITH GAP DETECTION
import asyncio
import logging
import signal
import sys
import os
from contextlib import asynccontextmanager
from typing import List, Dict, Any
import json
import time

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse
import torch

from config.settings import Settings
from config.logging_config import setup_logging
from src.websocket_handler import WebSocketManager
from src.conversation_manager import ConversationManager
from src.utils import get_system_info

# Import the FINAL FIXED model manager
from src.model_loader import VoxtralModelManager

try:
    from src.audio_processor import FixedAudioProcessor as AudioProcessor
except ImportError:
    from src.audio_processor import AudioProcessor

# Initialize logging
setup_logging()
logger = logging.getLogger(__name__)

# Global settings
settings = Settings()

# Global managers
model_manager = None
ws_manager = WebSocketManager()
conversation_manager = ConversationManager(max_turns=30, context_window_minutes=15)
audio_processor = AudioProcessor(
    sample_rate=16000,
    channels=1,
    chunk_duration_ms=30,
    conversation_manager=conversation_manager
)

# Shutdown flag
shutdown_event = asyncio.Event()

def signal_handler(signum, frame):
    """Handle shutdown signals"""
    logger.info(f"üõë Received signal {signum}, initiating graceful shutdown...")
    shutdown_event.set()

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FINAL FIX: Application lifespan with proper cleanup"""
    global model_manager
    
    # Startup
    logger.info("üöÄ Starting COMPLETELY FIXED Voxtral Real-Time Server...")
    
    try:
        model_manager = VoxtralModelManager(
            model_name=settings.MODEL_NAME,
            device=settings.DEVICE,
            torch_dtype=settings.TORCH_DTYPE
        )
        await model_manager.load_model()
        logger.info("‚úÖ FINAL FIXED model loaded successfully!")
    except Exception as e:
        logger.error(f"‚ùå Failed to load FINAL FIXED model: {e}")
        raise RuntimeError(f"FINAL FIXED model loading failed: {e}")
    
    # Start background cleanup
    cleanup_task = asyncio.create_task(background_cleanup())
    
    yield
    
    # Shutdown
    logger.info("üõë Shutting down FINAL FIXED server...")
    shutdown_event.set()
    
    # Cancel background tasks
    cleanup_task.cancel()
    try:
        await cleanup_task
    except asyncio.CancelledError:
        pass
    
    # Cleanup managers
    if model_manager:
        await model_manager.cleanup()
    await audio_processor.cleanup()
    
    logger.info("‚úÖ FINAL FIXED graceful shutdown completed")

async def background_cleanup():
    """Background maintenance tasks"""
    while not shutdown_event.is_set():
        try:
            await asyncio.sleep(300)  # Every 5 minutes
            active_connections = ws_manager.connection_count
            logger.debug(f"üîÑ FINAL FIXED background cleanup: {active_connections} active")
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"FINAL FIXED background cleanup error: {e}")

# Create FastAPI app
app = FastAPI(
    title="Voxtral Mini 3B - CONTINUOUS STREAMING Real-Time API",
    description="CONTINUOUS STREAMING with 300ms gap detection for understanding mode",
    version="7.0.0-CONTINUOUS-STREAMING",
    lifespan=lifespan
)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
async def root():
    """Serve the main test client page"""
    with open("static/index.html", "r") as f:
        html_content = f.read()
    return HTMLResponse(content=html_content)

@app.get("/health")
async def health_check():
    """FINAL FIXED: Health check endpoint"""
    try:
        system_info = get_system_info()
        model_status = "loaded" if model_manager and model_manager.is_loaded else "not_loaded"
        
        return {
            "status": "healthy",
            "model_status": model_status,
            "active_connections": ws_manager.connection_count,
            "conversation_sessions": len(conversation_manager.conversations),
            "architecture": "CONTINUOUS_STREAMING_WITH_GAP_DETECTION",
            "fixes_applied": [
                "‚úÖ CONTINUOUS STREAMING: Understanding mode now uses continuous streaming",
                "‚úÖ 300MS GAP DETECTION: WebRTC VAD detects 300ms silence gaps",  
                "‚úÖ ROBUST FFMPEG: Enhanced error handling and auto-restart",
                "‚úÖ FAST LLM: Optimized for <200ms response time",
                "‚úÖ NO CHUNK PROCESSING: Fixed individual chunk processing issue"
            ],
            "system": system_info,
            "shutdown_requested": shutdown_event.is_set(),
            "timestamp": asyncio.get_event_loop().time(),
            "continuous_streaming": True
        }
    except Exception as e:
        logger.error(f"FINAL FIXED health check failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/model/info")
async def model_info():
    """FINAL FIXED: Model information"""
    if not model_manager or not model_manager.is_loaded:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return {
        "model_name": settings.MODEL_NAME,
        "model_size": "3B parameters", 
        "device": str(settings.DEVICE),
        "dtype": str(settings.TORCH_DTYPE),
        "context_length": "32K tokens",
        "architecture": "CONTINUOUS_STREAMING_WITH_GAP_DETECTION",
        "supported_languages": [
            "English (en)", "Spanish (es)", "French (fr)", "Portuguese (pt)", 
            "Hindi (hi)", "German (de)", "Dutch (nl)", "Italian (it)"
        ],
        "mode_details": {
            "transcription": {
                "purpose": "Pure speech-to-text conversion",
                "output": "Exact words spoken by user",
                "temperature": 0.0,
                "api_method": "apply_transcription_request",
                "streaming": "continuous"
            },
            "understanding": {
                "purpose": "Conversational AI responses with gap detection",
                "output": "AI assistant responses after 300ms silence gap",
                "temperature": 0.3,
                "api_method": "continuous_streaming ‚Üí gap_detection ‚Üí transcribe ‚Üí respond",
                "streaming": "continuous_with_gap_detection",
                "gap_threshold": "300ms",
                "target_latency": "<200ms"
            }
        },
        "continuous_streaming_fixes": [
            "‚úÖ Understanding mode uses continuous WebM streaming",
            "‚úÖ WebRTC VAD with 300ms silence gap detection", 
            "‚úÖ Robust FFmpeg process with auto-restart",
            "‚úÖ Optimized LLM inference for <200ms latency",
            "‚úÖ Fixed audio chunk processing pipeline"
        ]
    }

@app.websocket("/ws/transcribe")
async def websocket_transcribe(websocket: WebSocket):
    """FINAL FIXED: WebSocket transcription - PURE SPEECH-TO-TEXT ONLY"""
    await ws_manager.connect(websocket, "transcribe")
    conversation_manager.start_conversation(websocket)
    
    try:
        logger.info("üé§ FINAL FIXED transcription session started")
        
        while not shutdown_event.is_set():
            try:
                # Receive audio data
                data = await websocket.receive_bytes()
                
                if shutdown_event.is_set():
                    await websocket.send_json({"info": "Server shutting down"})
                    break
                
                # FINAL FIX: Better data validation
                if not data or len(data) < 200:
                    logger.debug("Invalid/insufficient audio data received")
                    continue
                
                # Process through audio processor
                result = await audio_processor.process_webm_chunk_transcribe(data, websocket)
                
                # FINAL FIX: Better result validation with strict error handling
                if result and isinstance(result, dict):
                    if "error" in result:
                        logger.error(f"FINAL FIXED audio processing error: {result['error']}")
                        await websocket.send_json({
                            "error": f"Audio processing failed: {result['error']}", 
                            "final_fixed": True
                        })
                        continue
                    
                    if "audio_data" in result:
                        duration_ms = result.get("duration_ms", 0)
                        speech_ratio = result.get("speech_ratio", 0)
                        
                        # FINAL FIX: Better quality thresholds for human speech
                        if duration_ms > 1000 and speech_ratio > 0.4:
                            logger.info(f"üé§ FINAL FIXED TRANSCRIBING: {duration_ms:.0f}ms, speech: {speech_ratio:.3f}")
                            
                            if model_manager and model_manager.is_loaded:
                                # FINAL FIX: PURE transcription - NO response generation
                                transcription_result = await model_manager.transcribe_audio_pure(
                                    result["audio_data"], 
                                    language="en"
                                )
                                
                                # FINAL FIX: Strict validation - ONLY accept pure transcription
                                if (isinstance(transcription_result, dict) and 
                                    transcription_result.get("text") and
                                    "error" not in transcription_result and
                                    len(transcription_result["text"].strip()) > 1):
                                    
                                    # Add to conversation
                                    conversation_manager.add_turn(
                                        websocket,
                                        transcription=transcription_result["text"],
                                        audio_duration=duration_ms / 1000,
                                        speech_ratio=speech_ratio,
                                        mode="transcribe",
                                        language=transcription_result.get("language", "en")
                                    )
                                    
                                    # Add stats and send response
                                    conv_stats = conversation_manager.get_conversation_stats(websocket)
                                    transcription_result["conversation"] = conv_stats
                                    transcription_result["final_fixed"] = True
                                    
                                    await websocket.send_json(transcription_result)
                                    logger.info(f"‚úÖ FINAL FIXED PURE TRANSCRIBED: '{transcription_result['text']}'")
                                else:
                                    logger.debug(f"Filtered out non-speech or AI response: {transcription_result}")
                            else:
                                await websocket.send_json({"error": "Model not loaded"})
                        else:
                            logger.debug(f"Skipping: duration={duration_ms:.0f}ms, speech={speech_ratio:.3f}")
                    
            except WebSocketDisconnect:
                break
            except Exception as inner_e:
                logger.error(f"FINAL FIXED inner WebSocket transcription error: {inner_e}", exc_info=True)
                try:
                    if not shutdown_event.is_set():
                        await websocket.send_json({
                            "error": f"Processing error: {str(inner_e)}",
                            "final_fixed": True
                        })
                except:
                    break
                        
    except WebSocketDisconnect:
        logger.info("FINAL FIXED transcription WebSocket disconnected")
    except Exception as e:
        logger.error(f"FINAL FIXED WebSocket transcription error: {e}")
    finally:
        conversation_manager.cleanup_conversation(websocket)
        ws_manager.disconnect(websocket)

@app.websocket("/ws/understand")
async def websocket_understand(websocket: WebSocket):
    """CONTINUOUS STREAMING: Understanding mode with 300ms gap detection"""
    await ws_manager.connect(websocket, "understand")
    conversation_manager.start_conversation(websocket)
    
    try:
        logger.info("üß† CONTINUOUS STREAMING understanding session started")
        
        # Start continuous audio processing for understanding mode
        await audio_processor.start_ffmpeg_decoder("understand", websocket)
        
        # Store accumulated audio for gap-based processing
        understanding_context = {
            "accumulated_audio": bytearray(),
            "last_speech_time": time.time(),
            "processing_audio": False,
            "silence_start": None
        }
        
        # Background task for gap detection and LLM processing
        async def process_understanding_gaps():
            """Process accumulated audio when 300ms gap detected"""
            while not shutdown_event.is_set():
                try:
                    await asyncio.sleep(0.05)  # Check every 50ms
                    
                    current_time = time.time()
                    
                    # Check if we have accumulated audio and detected a 300ms gap
                    if (len(understanding_context["accumulated_audio"]) > 16000 and  # At least 1 second
                        not understanding_context["processing_audio"]):
                        
                        # Check for 300ms silence gap
                        silence_duration = current_time - understanding_context["last_speech_time"]
                        
                        if silence_duration >= 0.3:  # 300ms gap detected
                            understanding_context["processing_audio"] = True
                            
                            try:
                                # Get accumulated audio
                                audio_data = bytes(understanding_context["accumulated_audio"])
                                understanding_context["accumulated_audio"].clear()
                                
                                logger.info(f"üß† GAP DETECTED: Processing {len(audio_data)} bytes after {silence_duration:.3f}s gap")
                                
                                # Create WAV from accumulated PCM
                                wav_data = audio_processor._pcm_to_wav_enhanced(audio_data)
                                duration_ms = len(audio_data) / 2 / 16000 * 1000
                                
                                if model_manager and model_manager.is_loaded:
                                    # Get conversation context
                                    context = conversation_manager.get_conversation_context(websocket)
                                    
                                    # STEP 1: Transcribe the accumulated audio
                                    start_time = time.time()
                                    
                                    transcription_result = await model_manager.transcribe_audio_pure(
                                        wav_data,
                                        language="en"
                                    )
                                    
                                    transcribe_time = (time.time() - start_time) * 1000
                                    
                                    if (isinstance(transcription_result, dict) and 
                                        transcription_result.get("text") and 
                                        "error" not in transcription_result and
                                        len(transcription_result["text"].strip()) > 2):
                                        
                                        transcribed_text = transcription_result["text"].strip()
                                        logger.info(f"‚úÖ TRANSCRIBED in {transcribe_time:.0f}ms: '{transcribed_text}'")
                                        
                                        # STEP 2: Generate understanding response
                                        llm_start_time = time.time()
                                        
                                        understanding_result = await model_manager.generate_understanding_response(
                                            transcribed_text=transcribed_text,
                                            user_query="Please respond naturally to what I said",
                                            context=context
                                        )
                                        
                                        llm_time = (time.time() - llm_start_time) * 1000
                                        total_time = (time.time() - start_time) * 1000
                                        
                                        if (isinstance(understanding_result, dict) and 
                                            understanding_result.get("response") and 
                                            "error" not in understanding_result):
                                            
                                            # Send response
                                            final_result = {
                                                "type": "understanding",
                                                "transcription": transcribed_text,
                                                "response": understanding_result["response"],
                                                "timestamp": current_time,
                                                "language": transcription_result.get("language", "en"),
                                                "audio_quality": {
                                                    "duration_ms": duration_ms,
                                                    "gap_detected_ms": silence_duration * 1000
                                                },
                                                "performance": {
                                                    "transcribe_time_ms": transcribe_time,
                                                    "llm_time_ms": llm_time,
                                                    "total_time_ms": total_time
                                                },
                                                "continuous_streaming": True
                                            }
                                            
                                            # Add to conversation
                                            conversation_manager.add_turn(
                                                websocket,
                                                transcription=transcribed_text,
                                                response=understanding_result["response"],
                                                audio_duration=duration_ms / 1000,
                                                speech_ratio=0.8,  # Assume good quality since gap detected
                                                mode="understand",
                                                language=transcription_result.get("language", "en")
                                            )
                                            
                                            await websocket.send_json(final_result)
                                            logger.info(f"‚úÖ UNDERSTANDING RESPONSE in {total_time:.0f}ms (target: <200ms): '{understanding_result['response'][:50]}...'")
                                        else:
                                            logger.warning(f"Invalid understanding result: {understanding_result}")
                                    else:
                                        logger.warning(f"Invalid transcription result: {transcription_result}")
                                        
                            except Exception as processing_error:
                                logger.error(f"Gap processing error: {processing_error}", exc_info=True)
                                await websocket.send_json({
                                    "error": f"Gap processing failed: {str(processing_error)}",
                                    "continuous_streaming": True
                                })
                            finally:
                                understanding_context["processing_audio"] = False
                
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Gap detection error: {e}")
        
        # Start gap detection task
        gap_task = asyncio.create_task(process_understanding_gaps())
        
        while not shutdown_event.is_set():
            try:
                # Receive continuous WebM audio stream (like transcription mode)
                data = await websocket.receive_bytes()
                
                if shutdown_event.is_set():
                    await websocket.send_json({"info": "Server shutting down"})
                    break
                
                if not data or len(data) < 200:
                    continue
                
                # Process WebM chunk through audio processor to get PCM
                result = await audio_processor.process_webm_chunk_understand(data, websocket)
                
                if result and isinstance(result, dict) and "pcm_data" in result:
                    # Accumulate PCM data for gap-based processing
                    pcm_data = result["pcm_data"]
                    understanding_context["accumulated_audio"].extend(pcm_data)
                    
                    # Update last speech time if speech detected
                    if result.get("speech_detected", False):
                        understanding_context["last_speech_time"] = time.time()
                    
                    # Limit buffer size (30 seconds max)
                    max_buffer_size = 16000 * 30 * 2  # 30 seconds
                    if len(understanding_context["accumulated_audio"]) > max_buffer_size:
                        excess = len(understanding_context["accumulated_audio"]) - max_buffer_size
                        del understanding_context["accumulated_audio"][:excess]
                
            except WebSocketDisconnect:
                break
            except Exception as inner_e:
                logger.error(f"CONTINUOUS STREAMING inner error: {inner_e}", exc_info=True)
                try:
                    if not shutdown_event.is_set():
                        await websocket.send_json({
                            "error": f"Continuous streaming error: {str(inner_e)}",
                            "continuous_streaming": True
                        })
                except:
                    break
        
        # Cancel gap detection task
        gap_task.cancel()
        try:
            await gap_task
        except asyncio.CancelledError:
            pass
                
    except WebSocketDisconnect:
        logger.info("CONTINUOUS STREAMING understanding WebSocket disconnected")
    except Exception as e:
        logger.error(f"CONTINUOUS STREAMING WebSocket understanding error: {e}")
    finally:
        conversation_manager.cleanup_conversation(websocket)
        ws_manager.disconnect(websocket)

@app.get("/conversations")
async def get_conversations():
    """Get conversation statistics"""
    active_conversations = {}
    for conn_id, turns in conversation_manager.conversations.items():
        if turns:
            active_conversations[conn_id] = {
                "turns": len(turns),
                "last_activity": turns[-1].timestamp.isoformat(),
                "languages": list(set(turn.language for turn in turns if turn.language)),
                "total_duration": sum(turn.audio_duration for turn in turns),
                "modes": list(set(turn.mode for turn in turns))
            }
    
    return {
        "active_conversations": len(active_conversations),
        "total_ws_connections": ws_manager.connection_count,
        "conversation_details": active_conversations,
        "system_stats": {
            "max_turns_per_conversation": conversation_manager.max_turns,
            "context_window_minutes": conversation_manager.context_window.total_seconds() / 60,
            "audio_processor_stats": audio_processor.get_stats()
        },
        "continuous_streaming": True
    }

@app.post("/conversations/reset")
async def reset_conversations():
    """Reset all conversation data"""
    conversation_manager.conversations.clear()
    conversation_manager.language_patterns.clear()
    conversation_manager.audio_context.clear()
    audio_processor.reset()
    
    return {
        "status": "All conversations and audio processor reset successfully",
        "continuous_streaming": True
    }

@app.get("/debug/continuous-streaming")
async def debug_continuous_streaming():
    """CONTINUOUS STREAMING: Enhanced debug information"""
    return {
        "architecture": "CONTINUOUS_STREAMING_WITH_300MS_GAP_DETECTION",
        "conversation_manager": {
            "active_sessions": len(conversation_manager.conversations),
            "language_patterns": {k: v[-3:] for k, v in conversation_manager.language_patterns.items()},
        },
        "audio_processor": audio_processor.get_stats(),
        "websocket_manager": ws_manager.get_connection_stats(),
        "model_status": {
            "loaded": model_manager.is_loaded if model_manager else False,
            "memory_usage": model_manager._get_memory_usage() if model_manager and model_manager.is_loaded else {},
            "supported_languages": model_manager.supported_languages if model_manager else {},
            "default_language": getattr(model_manager, 'default_language', 'en') if model_manager else 'en'
        },
        "system_status": {
            "shutdown_requested": shutdown_event.is_set(),
            "background_tasks_active": not shutdown_event.is_set()
        },
        "continuous_streaming_architecture": {
            "transcription_mode": "Continuous WebM streaming ‚Üí FFmpeg ‚Üí PCM ‚Üí VAD ‚Üí LLM",
            "understanding_mode": "Continuous WebM streaming ‚Üí PCM accumulation ‚Üí 300ms gap detection ‚Üí LLM",
            "gap_detection": "WebRTC VAD with 300ms silence threshold",
            "target_latency": "<200ms for LLM response",
            "audio_buffer": "30 second max accumulation",
            "ffmpeg_robustness": "Auto-restart with enhanced error handling"
        },
        "fixes_implemented": [
            "‚úÖ CONTINUOUS STREAMING: Understanding mode now continuous like transcription",
            "‚úÖ 300MS GAP DETECTION: WebRTC VAD detects silence gaps properly",
            "‚úÖ ROBUST FFMPEG: Enhanced process management and auto-restart",
            "‚úÖ PCM ACCUMULATION: Proper audio buffering for gap-based processing", 
            "‚úÖ FAST LLM: Optimized inference pipeline for <200ms latency",
            "‚úÖ NO CHUNK PROCESSING: Fixed individual chunk processing bug"
        ],
        "continuous_streaming": True
    }

if __name__ == "__main__":
    try:
        uvicorn.run(
            "src.main:app",
            host="0.0.0.0",
            port=8000,
            reload=False,
            log_level="info",
            access_log=True,
            timeout_graceful_shutdown=30
        )
    except KeyboardInterrupt:
        logger.info("üõë CONTINUOUS STREAMING server stopped by user")
    except Exception as e:
        logger.error(f"‚ùå CONTINUOUS STREAMING server error: {e}")
        sys.exit(1)
